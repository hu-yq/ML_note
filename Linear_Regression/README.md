# 线性回归

+ 定义： 线性回归是利用回归方程（函数）对一个或多个自变量（特征值）和因变量（目标值）之间的关系进行建模的一种分析方式

+ 特点：只有一个自变量的情况称为单变量回归，对于多个自变量的情况称为多元回归

$$
h(x) = w_1*x_1 + w_2*x_2 + w_3*x_3 + w_4*x_4 + b = W^T*x
$$

$$
h(x) = w_1*x_1 + w_2*x_1^2 + w_3*x_1^3 + w_4*x_1^4 + b = W^T*x^{(i)} = W^T*x^{'}
$$

其中$w$,$x$可以理解为矩阵：

$$
\begin{aligned}
w=\begin{pmatrix} w_1\\w_2\\b\end{pmatrix},x=\begin{pmatrix} x_1\\x_2\\b\end{pmatrix}
\end{aligned}
$$

- 假设误差$\epsilon^{(i)}$服从均值为0，方差为$\theta^2$的正态分布，独立同分布

$$
p(\epsilon^{(i)}) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(\epsilon^{(i)})^2}{2\sigma^2}}
$$
$$
p(y^{(i)}|x^{(i)},\theta) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}}
$$
$$
\begin{aligned}
 L(\theta) &= \prod_{i=1}^m p(y^{(i)}|x^{(i)},\theta) \\
           &= \prod_{i=1}^m \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}}
\end{aligned}
$$
$$
\begin{aligned}
  ℓ(\theta) &= logL(\theta) \\
  &=log \prod_{i=1}^m \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}} \\
  &= \sum_{i=1}^m log \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}} \\ 
  &= m log \frac{1}{\sqrt{2\pi}\sigma} - \frac{1}{\sigma^2} \frac{1}{2} \sum_{i=1}^m(y^{(i)}-\theta^Tx^{(i)})^2
\end{aligned}
$$

令
$$
J(\theta) = \frac{1}{2} \sum_{i=1}^m(y^{(i)}-\theta^Tx^{(i)})^2 = \frac{1}{2} \sum_{i=1}^m(h(x^{(i)}) - y^{(i)})^2
$$

## 损失函数与优化

总损失函数定义：
$$
\begin{aligned}
J(w)&=(h(x^{(1)})-y^{(1)})^2+(h(x^{(2)})-y^{(2)})^2+(h(x^{(3)})-y^{(3)})^2+...+(h(x^{(n)})-y^{(n)})^2\\
	&=\sum_{i=1}^n(h(x^{(i)})-y^{(i)})^2
\end{aligned}
$$

* y 为第i个训练样本的真实值 

* h(x)为第i个训练样本特征值组合预测函数 

- ⼜称最⼩⼆乘法

### 方法一：正规方程

$$
w=(X^TX)^{-1}X^Ty
$$

+ 理解：X为特征值矩阵，y为⽬标值矩阵。直接求到最好的结果 

+ 缺点：当特征过多过复杂时，求解速度太慢并且得不到结果

#### 公式推导

$$
\begin{aligned}
J(w)&=(h(x^{(1)})-y^{(1)})^2+(h(x^{(2)})-y^{(2)})^2+(h(x^{(3)})-y^{(3)})^2+...+(h(x^{(n)})-y^{(n)})^2\\
	&=\sum_{i=1}^n(h(x^{(i)})-y^{(i)})^2 \\
	&=(Xw-y)^2
\end{aligned}
$$

其中y是真实值矩阵，X是特征值矩阵，w是权重矩阵 

对其求解关于w的最⼩值，起⽌y,X 均已知⼆次函数直接求导，导数为零的位置，即为最⼩值。

对w求导：

$$
\begin{aligned}
2(Xw-y)*X  =&  0 \\
2(Xw-y)*(XX^T)  =&  0X^T \\
2(Xw-y)*(XX^T)(XX^T)^{-1}  =&  0X^T(XX^T)^{-1} \\
2(Xw-y) =& 0 \\
Xw =& y \\
X^TXw =& X^Ty \\
(X^TX)^{-1}X^TXw =& (X^TX)^{-1}X^Ty \\
w =&(X^TX)^{-1}X^Ty \\
\end{aligned}
$$

#### 另一种推导方式

把损失函数分开书写：
$$
(Xw-y)^2 = (Xw-y)^T(Xw-y)
$$
对展开上式进⾏求导：
$$
\begin{aligned}
\frac{\partial}{\partial{w}}[(Xw-y)^T(Xw-y)]=&\frac{\partial}{\partial{w}}[(X^Tw^T-y^T)(Xw-y)] \\
=&\frac{\partial}{\partial{w}}(w^TX^TXw-w^TX^Ty-y^TXw+y^Ty) \\
=&X^TXw+X^TXw-X^Ty-X^Ty \\
\end{aligned}
$$

需要求得求导函数的极⼩值，即上式求导结果为0，经过化解，得结果为：
$$
\begin{aligned}
X^TXw+X^TXw-X^Ty-X^Ty &= 0 \\
X^TXw&=X^Ty \\
w&=(X^TX)^{-1}X^Ty
\end{aligned}
$$

>  补充矩阵求导公式：
> $$
> \frac{{\rm d}x^TA}{{\rm d}x} = A 
> $$
> $$
> \frac{{\rm d}Ax}{{\rm d}x} = A^T 
> $$
> $$
> \frac{{\rm d}x^TAx}{{\rm d}x} = (A+A^T)x 
> $$
> 



### 方法二：梯度下降

梯度是微积分中⼀个很重要的概念 :

- 在单变量的函数中，梯度其实就是函数的微分，代表着函数在某个给定点的切线的斜率；

- 在多变量函数中，梯度是⼀个向量，向量有⽅向，梯度的⽅向就指出了函数在给定点的上升最快的⽅向；

> 在微积分⾥⾯，对多元函数的参数求∂偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是 梯度。

#### 梯度下降（Gradient Descent）公式

$$
\theta_{i+1} = \theta_i-\alpha\frac{\partial}{\partial\theta_i}J(\theta)
$$

- α在梯度下降算法中被称作为学习率或者步⻓;

- 梯度前加⼀个负号，就意味着朝着梯度相反的⽅向前进！我们在前⽂提到，梯度的⽅向实际就是函数在此点上升最快的 ⽅向！⽽我们需要朝着下降最快的⽅向⾛，⾃然就是负的梯度的⽅向，所以此处需要加上负号

#### 梯度下降法推导流程

##### 先决条件：确定优化模型的假设函数和损失函数

⽐如对于线性回归，假设函数表示为 $h_{\theta}(x_1,x_2, ...,x_n) = θ_0 + θ_1x_1 + ... + θ_nx_n$ , 其中$\theta_i(i = 0, 1, 2...n)$为模型参数， $x_i(i = 0, 1, 2...n)$为每个样本的n个特征值。这个表示可以简化，我们增加⼀个特征x = 1 ，这样
$$
h_\theta(x_0,x_1,...,x_n)=\sum_{i=0}^n\theta_ix_i
$$
同样是线性回归，对应于上⾯的假设函数，损失函数为：
$$
J(x_0,x_1,...,x_n) = \frac{1}{2m}\sum_{j=0}^m(h_\theta(x_0^{(j)},x_1^{(j)},...,x_n)^{(j)} - y_j)^2
$$

##### 算法相关参数初始化

主要是初始化 $θ_0, θ_1...,θ_n$ ,算法终⽌距离 $ε$ 以及步⻓ $α$ 。在没有任何先验知识的时候，将所有的$θ$初始化为0， 将步 ⻓初始化为1。在调优的时候再优化。

##### 算法过程

1. 确定当前位置的损失函数的梯度，对于$\theta_i$ , 其梯度表达式如下：
   $$
   \frac{\partial}{\partial\theta_i}J(x_0,x_1,...,x_n)
   $$

2. ⽤步⻓乘以损失函数的梯度，得到当前位置下降的距离，即
   $$
   \alpha\frac{\partial}{\partial\theta_i}J(x_0,x_1,...,x_n)
   $$

3.  确定是否所有的$θ$ ,梯度下降的距离都⼩于$ε$，如果⼩于$ε$则算法终⽌，当前所有的$θ_i(i = 0, 1, ...n)$即为最终结果。否 则进⼊步骤4

4. 更新所有的$θ $，对于$θ $，其更新表达式如下。更新完毕后继续转⼊步骤1
   $$
   \theta_i =\theta_i - \alpha\frac{\partial}{\partial\theta_i}J(x_0,x_1,...,x_n)x_i^{(j)}
   $$
   
#### 几种梯度下降算法

1. 全梯度下降算法 (Full gradient descent)
2. 随机梯度下降算法 (Stochastic gradient descent)
3. 小批量梯度下降算法 (Mini-batch gradient descent)
4. 随机平均梯度下降算法 (Stochastic average gradient descent)
   
##### 全梯度下降算法（FG)

批量梯度下降法，是梯度下降法最常⽤的形式，具体做法也就是在更新参数时使⽤所有的样本来进⾏更新。
计算训练集所有样本误差，对其求和再取平均值作为⽬标函数。
权重向量沿其梯度相反的⽅向移动，从⽽使当前⽬标函数减少得最多。
其是在整个训练数据集上计算损失函数关于参数$θ$的梯度：

$$
\theta_i =\theta_i - \alpha\sum_{j=1}^m(h_\theta(x_0^{(j)},x_1^{(j)},...,x_n^{(j)})-y_j)x_i^{(j)}
$$

注意：
- 因为在执⾏每次更新时，我们需要在整个数据集上计算所有的梯度，所以批梯度下降法的速度会很慢，同时，批梯度下降法⽆法处理超出内存容量限制的数据集。
- **批梯度下降法同样也不能在线更新模型，即在运⾏的过程中，不能增加新的样本**。

##### 随机梯度下降算法 (SG)

由于FG每迭代更新⼀次权重都需要计算所有样本误差，⽽实际问题中经常有上亿的训练样本，故效率偏低，且容易陷⼊局部最优解，因此提出了随机梯度下降算法。

其每轮计算的⽬标函数不再是全体样本误差，⽽仅是单个样本误差，即**每次只代⼊计算⼀个样本⽬标函数的梯度来更新权重，再取下⼀个样本重复此过程，直到损失函数值停⽌下降或损失函数值⼩于某个可以容忍的阈值**。

此过程简单，⾼效，通常可以较好地避免更新迭代收敛到局部最优解。其迭代形式为

$$
\theta_i =\theta_i - \alpha(h_\theta(x_0^{(j)},x_1^{(j)},...,x_n^{(j)})-y_j)x_i^{(j)}
$$

但是由于，SG每次只使⽤⼀个样本迭代，若遇上噪声则容易陷⼊局部最优解

##### 小批量梯度下降算法 (Mini-batch) 

⼩批量梯度下降算法是FG和SG的折中⽅案,在⼀定程度上兼顾了以上两种⽅法的优点。

**每次从训练样本集上随机抽取⼀个⼩样本集，在抽出来的⼩样本集上采⽤FG迭代更新权重。**

被抽出的⼩样本集所含样本点的个数称为batch_size，通常设置为2的幂次⽅，更有利于GPU加速处理。

特别的，若batch_size=1，则变成了SG；若batch_size=n，则变成了FG.其迭代形式为

$$
\theta_i =\theta_i - \alpha\sum_{j=t}^{t+x-1}(h_\theta(x_0^{(j)},x_1^{(j)},...,x_n^{(j)})-y_j)x_i^{(j)}
$$

##### 随机平均梯度下降算法 (SAG)

在SG⽅法中，虽然避开了运算成本⼤的问题，但对于⼤数据训练⽽⾔，SG效果常不尽如⼈意，因为每⼀轮梯度更新都完全与上⼀轮的数据和梯度⽆关。

**随机平均梯度算法克服了这个问题，在内存中为每⼀个样本都维护⼀个旧的梯度，随机选择第i个样本来更新此样本的梯度，其他样本的梯度保持不变，然后求得所有梯度的平均值，进⽽更新了参数**。

其迭代形式为：

$$
\theta_i = \theta_i - \frac{\alpha}{n}(h_\theta(x_0^{(j)},x_1^{(j)},...,x_n^{(j)})-y_j))x_i^{(j)}
$$

- 我们知道sgd是当前权重减去步⻓乘以梯度，得到新的权重。sag中的a，就是平均的意思，具体说，就是在第k步迭代的时候，我考虑的这⼀步和前⾯n-1个梯度的平均值，当前权重减去步⻓乘以最近n个梯度的平均值。
- n是⾃⼰设置的，当n=1的时候，就是普通的sgd。
- 这个想法⾮常的简单，在随机中⼜增加了确定性，类似于mini-batch sgd的作⽤，但不同的是，sag⼜没有去计算更多的样本，只是利⽤了之前计算出来的梯度，所以每次迭代的计算成本远⼩于mini-batch sgd，和sgd相当。效果⽽⾔，sag相对于sgd，收敛速度快了很多。这⼀点下⾯的论⽂中有具体的描述和证明。
- [SAG论文链接](https://arxiv.org/pdf/1309.2388.pdf)


## 过拟合与欠拟合

### 定义
- 过拟合：⼀个假设在训练数据上能够获得⽐其他假设更好的拟合， 但是在测试数据集上却不能很好地拟合数据，此时认为这个假设出现了过拟合的现象。(模型过于复杂)
- ⽋拟合：⼀个假设在训练数据上不能获得更好的拟合，并且在测试数据集上也不能很好地拟合数据，此时认为这个假设出现了⽋拟合的现象。(模型过于简单)

![过拟合和欠拟合](https://github.com/hu-yq/Machine-learning-algorithm-finishing-/blob/master/Linear_Regression/01.png)

### 原因及解决办法

- ⽋拟合原因以及解决办法
   - 原因：学习到数据的特征过少
   - 解决办法：
      - 1）添加其他特征项，有时候我们模型出现⽋拟合的时候是因为特征项不够导致的，可以添加其他特征项来很好地解决。例如，“组合”、“泛化”、“相关性”三类特征是特征添加的重要⼿段，⽆论在什么场景，都可以照葫芦画瓢，总会得到意想不到的效果。除上⾯的特征之外，“上下⽂特征”、“平台特征”等等，都可以作为特征添加的⾸选项。
      - 2）添加多项式特征，这个在机器学习算法⾥⾯⽤的很普遍，例如将线性模型通过添加⼆次项或者三次项
      使模型泛化能⼒更强。

- 过拟合原因以及解决办法
   - 原因：原始特征过多，存在⼀些嘈杂特征， 模型过于复杂是因为模型尝试去兼顾各个测试数据点
   - 解决办法：
      - 1）重新清洗数据，导致过拟合的⼀个原因也有可能是数据不纯导致的，如果出现了过拟合就需要我们重新清洗数据。
      - 2）增⼤数据的训练量，还有⼀个原因就是我们⽤于训练的数据量太⼩导致的，训练数据占总数据的⽐例过⼩。
      - 3）正则化
      - 4）减少特征维度，防⽌维灾难

### 正则化

定义：在学习的时候，数据提供的特征有些影响模型复杂度或者这个特征的数据点异常较多，所以算法在学习的时候尽量减少这个特征的影响（甚⾄删除某个特征的影响），这就是正则化

#### 正则化类别

- L2正则化
   - 作⽤：可以使得其中⼀些W的都很⼩，都接近于0，削弱某个特征的影响
   - 优点：越⼩的参数说明模型越简单，越简单的模型则越不容易产⽣过拟合现象
   - Ridge回归
- L1正则化
   - 作⽤：可以使得其中⼀些W的值直接为0，删除这个特征的影响
   - LASSO回归

### 正则化线性模型

#### Ridge Regression (岭回归，⼜名 Tikhonov regularization)

岭回归是线性回归的正则化版本，即在原来的线性回归的 cost function 中添加正则项（regularization term）:

$$
\alpha\sum_{i=1}^n\theta_i^2
$$

以达到在拟合数据的同时，使模型权重尽可能⼩的⽬的,岭回归代价函数:

$$
J(\theta)=MSE(\theta)+\alpha\sum_{i=1}^n\theta_i^2
$$

即:

$$
J(\theta)=\frac{1}{m}\sum_{i=1}^m(\theta^Tx^{(i)}-y^{(i)})^2+\alpha\sum_{i=1}^n\theta_i^2
$$

- α=0：岭回归退化为线性回归 


#### Lasso Regression(Lasso 回归)

Lasso 回归是线性回归的另⼀种正则化版本，正则项为权值向量的$ℓ1$范数。

Lasso回归的代价函数 ：
$$
J(\theta)=MSE(\theta)+\alpha\sum_{i=1}^n |\theta_i|
$$

【注意 】
- Lasso Regression 的代价函数在 θ =0处是不可导的
- 解决⽅法：在θ =0处⽤⼀个次梯度向量(subgradient vector)代替梯度，如下式
- Lasso Regression 的次梯度向量
  
$$
g(\theta,J)=\nabla_\theta MSE(\theta) + \alpha 
\begin{pmatrix} 
sign(\theta_1) \\ 
sign(\theta_2) \\ 
sign(\theta_3) 
\end{pmatrix} 
$$
where
$$
sign(\theta_i)= 
\begin{cases} 
-1 &  if \ \  \theta_i<0\\ 
0 &  if \ \  \theta_i=0\\ 
1 & if \ \  \theta_i>0
\end{cases}
$$


Lasso Regression 有⼀个很重要的性质是：**倾向于完全消除不重要的权重。**

例如：当α 取值相对较⼤时，⾼阶多项式退化为⼆次甚⾄是线性：⾼阶多项式特征的权重被置为0。

也就是说，Lasso Regression 能够⾃动进⾏特征选择，并输出⼀个稀疏模型（只有少数特征的权重是⾮零的）。

#### Elastic Net (弹性⽹络)

弹性⽹络在岭回归和Lasso回归中进⾏了折中，通过混合⽐(mix ratio) r 进⾏控制：
- r=0：弹性⽹络变为岭回归
- r=1：弹性⽹络便为Lasso回归
  

弹性⽹络的代价函数 ：

$$
J(\theta)=MSE(\theta)+r\alpha\sum_{i=1}^n |\theta_i|+\frac{1-r}{2}\alpha\sum_{i=1}^n\theta_i^2
$$

