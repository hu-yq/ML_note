## 朴素贝叶斯（⽣成模型）

**一个重要假设：特征之间同等重要且相互独立**

### 联合概率、条件概率与相互独⽴ 

- 联合概率：包含多个条件，且所有条件同时成⽴的概率
    - 记作：$P(A,B)$
- 条件概率：就是事件A在另外⼀个事件B已经发⽣条件下的发⽣概率
    - 记作：$P(A|B)$
- 相互独⽴：如果$P(A, B) = P(A)P(B)$，则称事件A与事件B相互独⽴。

### 公式 

$$
p(C|W) = \frac{p(C,W)}{p(W)} = \frac{p(W|C)P(C)}{p(W)}
$$ 

或者 

$$
p(C|F_1,F_2,...) = \frac{p(F_1,F_2,...,F_n|C)P(C)}{p(F_1,F_2,...,F_n)} = \frac{p(F_1|C)p(F_2|C)...p(Fn|C)P(C)}{p(F_1,F_2,...,F_n)}
$$

公式分为三个部分：
- $P(C)$：每个⽂档类别的概率(某⽂档类别数／总⽂档数量)
- $P(W│C)$：给定类别下特征（被预测⽂档中出现的词）的概率
    - 计算⽅法：$P(F_1│C)=N_i/N$ （训练⽂档中去计算）
        - $N_i$为该$F1$词在C类别所有⽂档中出现的次数
        - $N$为所属类别$C$下的⽂档所有词出现的次数和
- $P(F1,F2,…)$ 预测⽂档中每个词的概率

#### 拉普拉斯平滑
解决：某个特征不存在，$p(F_1|C) = 0$的情况

$$
p(F_1|C) = \frac{N_i+\alpha}{N + \alpha m}
$$
$\alpha$为指定的系数，一般为1，$m$为训练文档中统计出的特征词个数 


## 朴素⻉叶斯优缺点
- 优点：
    - 朴素⻉叶斯模型发源于古典数学理论，有稳定的分类效率
    - 对缺失数据不太敏感，算法也⽐较简单，常⽤于⽂本分类
    - 分类准确度⾼，速度快
- 缺点：
    - 由于使⽤了样本属性独⽴性的假设，所以如果特征属性有关联时其效果不好
    - 需要计算先验概率，⽽先验概率很多时候取决于假设，假设的模型可以有很多种，因此在某些时候会由于假设的先验模型的原因导致预测效果不佳；


## 朴素⻉叶斯与LR的区别

- 前者是⽣成式模型，后者是判别式模型，⼆者的区别就是⽣成式模型与判别式模型的区别。
    - ⾸先，Navie Bayes通过已知样本求得先验概率P(Y), 及条件概率P(X|Y), 对于给定的实例，计算联合概率，进⽽求出后验概率。也就是说，它尝试去找到底这个数据是怎么⽣成的（产⽣的），然后再进⾏分类。哪个类别最有可能产⽣这个信号，就属于那个类别。
        - 优点： 样本容量增加时，收敛更快；隐变量存在时也可适⽤。
        - 缺点：时间⻓；需要样本多；浪费计算资源

    - 相⽐之下，Logistic回归不关⼼样本中类别的⽐例及类别下出现特征的概率，它直接给出预测模型的式⼦。设每个特征都有⼀个权重，训练样本数据更新权重w，得出最终表达式。
        - 优点：
            - 直接预测往往准确率更⾼；
            - 简化问题；
            - 可以反应数据的分布情况，类别的差异特征；
            - 适⽤于较多类别的识别。
        - 缺点
            - 收敛慢；
            - 不适⽤于有隐变量的情况。
