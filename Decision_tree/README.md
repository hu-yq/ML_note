## 决策树

决策树思想的来源⾮常朴素，程序设计中的条件分⽀结构就是if-else结构，最早的决策树就是利⽤这类结构分割数据的
⼀种分类学习⽅法
- 决策树：
    - 是⼀种树形结构，本质是⼀颗由多个判断节点组成的树
    - 其中每个内部节点表示⼀个属性上的判断，
    - 每个分⽀代表⼀个判断结果的输出，
    - 最后每个叶节点代表⼀种分类结果。


### 熵 (Entropy)

用来描述系统的混乱程度，系统越有序，熵值越低；系统越混乱或者分散，熵值越⾼。 

"信息熵" (information entropy)是度量样本集合纯度最常⽤的⼀种指标。假定当前样本集合 $D$ 中第 $k$ 类样本所占的⽐例为 $p_k$ $(k = 1, 2,. . . , |y|)$ ，$p =\frac{C^k}{D}$ ,$D$为样本的所有数量，$C^k$ 为第$k$类样本的数量。则 $D$的信息熵定义为(（$log$是以2为底，$lg$是以10为底）:
$$
Ent(D) = -\sum_{k=1}^n \frac{C^k}{D} log \frac{C^k}{D} = -\sum_{k=1}^n p_k log_2 p_k = -p_1log_2 p_1 - p_2log_2 p_2 - ... - p_n log_n p_n
$$
其中：$Ent(D)$ 的值越⼩，则 $D$ 的纯度越⾼。

### 决策树的划分依据 --- 信息增益 

**信息增益**：以某特征划分数据集前后的熵的差值。熵可以表示样本集合的不确定性，熵越⼤，样本的不确定性就越⼤。因此可以使⽤划分前后集合熵的差值来衡量使⽤当前特征对于样本集合D划分效果的好坏。


信息增益 = entroy(前) - entroy(后)
> 注：信息增益表示得知特征X的信息⽽使得类Y的信息熵减少的程度

#### 定义与公式
假定离散属性$a$有 $V$ 个可能的取值: $a^1, a^2,..., a^V$
> 假设离散属性性别有2（男，⼥）个可能的取值


若使⽤$a$来对样本集 $D$ 进⾏划分，则会产⽣ $V$ 个分⽀结点,其中第$v$个分⽀结点包含了 $D$ 中所有在属性$a$上取值为$a$ 的样本，记为$D$ . 

我们可根据前⾯给出的信息熵公式计算出$D$的信息熵，再考虑到不同的分⽀结点所包含的样本数不同，给分⽀结点赋予权重 $\frac{D^v}{D}$ ，
即样本数越多的分⽀结点的影响越⼤，于是可计算出⽤属性a对样本集 D 进⾏划分所获得的"信息增益" (informationgain)

其中：

特征$a$对训练数据集$D$的信息增益$Gain(D,a)$,定义为集合$D$的信息熵$Ent(D)$与给定特征$a$条件下$D$的信息条件熵$Ent(D∣a)$之差，即公式为：
$$
Gain(D,a) = Ent(D) - Ent(D|a) = Ent(D) - \sum_{v=1}^V \frac{D^v}{D} Ent(D^v)
$$
$$
Ent(D) = -\sum_{k=1}^n \frac{C^k}{D} log \frac{C^k}{D}
$$
$$
Ent(D|a) = \sum_{v=1}^V \frac{D^v}{D} Ent(D^v) = - \sum_{v=1}^V \frac{D^v}{D} \sum_{k=1}^n \frac{C^{kv}}{D_v} log \frac{C^{kv}}{D_v}
$$
其中：

$D$ 表示$a$属性中第$v$个分⽀节点包含的样本数;

$C$ 表示$a$属性中第$v$个分⽀节点包含的样本数中，第$k$个类别下包含的样本数;

⼀般⽽⾔，信息增益越⼤，则意味着使⽤属性 $a$ 来进⾏划分所获得的"纯度提升"越⼤。因此，我们可⽤信息增益来进⾏决策树的划分属性选择，著名的 ID3 决策树学习算法 [Quinlan， 1986] 就是以信息增益为准则来选择划分属性。

**注意：信息增益准则对可取值数⽬较多的属性有所偏好, 只能对描述属性为离散型属性的数据集构造决策树。**

### 决策树的划分依据 --- 信息增益率 

**信息增益准则对可取值数⽬较多的属性有所偏好**，为减少这种偏好可能带来的不利影响，著名的 $C4.5$ 决策树算法 [Quinlan，1993] 不直接使⽤信息增益，⽽是使⽤"增益率" (gain ratio) 来选择最优划分属性。

增益率：增益率是⽤前⾯的信息增益$Gain(D, a)$和属性$a$对应的"固有值"(intrinsic value) [Quinlan , 1993] 的⽐值来共同定义的。

$$
Gain_ratio(D,a) = \frac{Gain(D,a)}{IV(a)}
$$
其中
$$
IV(a) = - \sum_{v=1}^V \frac{D^v}{D} log \frac{D^v}{D}
$$


##### 信息增益和信息增益率（$C4.5$）比较 ，为什么$C4.5$要好 
- 1.⽤信息增益率来选择属性
  
    克服了⽤信息增益来选择属性时偏向选择值多的属性的不⾜。

- 2.采⽤了⼀种后剪枝⽅法
  
    避免树的⾼度⽆节制的增⻓，避免过度拟合数据

- 3.对于缺失值的处理
  
    在某些情况下，可供使⽤的数据可能缺少某些属性的值。假如$〈x，c(x)〉$是样本集S中的⼀个训练实例，但是其属性$A$的值$A(x)$未知。

    处理缺少属性值的⼀种策略是赋给它结点$n$所对应的训练实例中该属性的最常⻅值；另外⼀种更复杂的策略是为$A$的每个可能值赋予⼀个概率。

    例如，给定⼀个布尔属性$A$，如果结点$n$包含6个已知$A=1$和4个$A=0$的实例，那么$A(x)=1$的概率是0.6，⽽$A(x)=0$的概率是0.4。于是，实例x的60%被分配到A=1的分⽀，40%被分配到另⼀个分⽀。

    $C4.5$就是使⽤这种⽅法处理缺少的属性值。


### 决策树的划分依据 --- 基尼值和基尼指数 

**基尼值**$Gini(D)$ ：从数据集$D$中随机抽取两个样本，其类别标记不⼀致的概率。故，$Gini（D）$值越⼩，数据集D的纯度越⾼。数据集 $D$ 的纯度可⽤基尼值来度量:
$$
Gini(D) = \sum_{k=1}^{|y|} \sum_{k \neq k} p_k p_k = 1-\sum_{k=1}^{|y|} p_{k}^2
$$

> $p_k=\frac{C^k}{D}$, $D$为样本的所有数量，$C^k$为第$k$类样本的数量

**基尼指数**$Gini_index(D)$：⼀般，选择使划分后基尼系数最⼩的属性作为最优化分属性。
$$
Gini_index(D,a) = \sum_{v=1}^V \frac{D^v}{D} Gini(D^v) 
$$

- - - 
#### 小结 

|名称|分支方式|备注
|:--|:--|:--
|ID3|信息增益|ID3只能对离散属性的数据集构成决策树
|C4.5|信息增益率|优化后解决了ID3分⽀过程中总喜欢偏向选择值较多的属性
|CART|Gini系数|可以进⾏分类和回归，可以处理离散属性，也可以处理连续属性

### cart剪枝 

剪枝 (pruning)是决策树学习算法对付"过拟合"的主要⼿段。

在决策树学习中，为了尽可能正确分类训练样本，结点划分过程将不断重复，有时会造成决策树分⽀过多，这时就可能因训练样本学得"太好"了，以致于把训练集⾃身的⼀些特点当作所有数据都具有的⼀般性质⽽导致过拟合。因此，可通过主动去掉⼀些分⽀来降低过拟合的⻛险。

#### 常⽤的减枝⽅法 

决策树剪枝的基本策略有"预剪枝" (pre-pruning)和"后剪枝"(post- pruning) 。

- 预剪枝是指在决策树⽣成过程中，对每个结点在划分前先进⾏估计，若当前结点的划分不能带来决策树泛化性能提升，则停⽌划分并将当前结点标记为叶结点;
- 后剪枝则是先从训练集⽣成⼀棵完整的决策树，然后⾃底向上地对⾮叶结点进⾏考察，若将该结点对应的⼦树替换为叶结点能带来决策树泛化性能提升，则将该⼦树替换为叶结点。

